{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "## from 02. Autograd-ìë™ë¯¸ë¶„ ~ \n",
    "20.03.31.tue.pm 6:12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List\n",
    "    * 02. Autograd\n",
    "    * 03. ì‹ ê²½ë§(Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>autograd</code>íŒ¨í‚¤ì§€ëŠ” Tensorì˜ ëª¨ë“  ì—°ì‚°ì— ëŒ€í•´ ìë™ ë¯¸ë¶„ì„ ì œê³µí•œë‹¤.<br>\n",
    "define-by-run í”„ë ˆì„ì›Œí¬ë¡œ, ì½”ë“œë¥¼ ì–´ë–»ê²Œ ì‘ì„±í•˜ì—¬ ì‹¤í–‰í•˜ëŠëƒì— ë”°ë¼ ì—­ì „íŒŒê°€ ì •ì˜ëœë‹¤ëŠ” ëœ»ì´ë©°, ì—­ì „íŒŒëŠ” í•™ìŠµ ê³¼ì •ì˜ ë§¤ ë‹¨ê³„ë§ˆë‹¤ ë‹¬ë¼ì§„ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorë¥¼ ìƒì„±í•˜ê³ , <code>requires_grad=True</code>ë¥¼ ì„¤ì •í•˜ì—¬ ì—°ì‚°ì„ ê¸°ë¡í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)  # requires_grad=Trueë¡œ ì„¤ì •í•˜ì—¬ ì—°ì‚°ì„ ê¸°ë¡í•œë‹¤.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# tensorì— ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.\n",
    "\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yëŠ” ì—°ì‚° ê²°ê³¼ë¡œ ìƒì„±ëœ ê²ƒì´ë¯€ë¡œ <code>.grad_fn</code>ì„ ê°–ëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000001B2C8E16948>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>.grad_fn</code>ì†ì„±ì€ <code>Tensor</code>ë¥¼ ìƒì„±í•œ <code>Function</code>ì„ ì°¸ì¡°í•˜ê³  ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>y</code>ì— ë‹¤ë¥¸ ì—°ì‚°ì„ ìˆ˜í–‰í•´ë³´ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)  # z ë˜í•œ .grad_fn ì†ì„±ì„ ê°€ì§€ê³  ìˆì„ê¹Œ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>.requires_grad_(...)</code>ëŠ” ê¸°ì¡´ Tensorì˜ <code>requires_grad</code>ê°’ì„ ë°”ê¿”ì¹˜ê¸° (in-place)í•˜ì—¬ ë³€ê²½í•œë‹¤.<br>\n",
    "(ê¸°ì¡´ requires_gradì˜€ë˜ xë¥¼ ë°”ê¿”ì¹˜ê¸° í•œë‹¤ëŠ” ì˜ë¯¸ì¸ê°€?)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aë¼ëŠ” ìƒˆë¡œìš´ tensorë¥¼ ë§Œë“¤ì–´ë³´ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>a.requires_grad</code>ë¥¼ ì¶œë ¥í•´ë³´ë©´ Falseë¡œ ì¶œë ¥ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.<br>\n",
    "<code>requires_grad</code>ë¥¼ Trueë¡œ ë³€ê²½í•˜ê¸° ìœ„í•´ <code>.requires_grad_(...)</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ê°’ì„ in-placeí•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9001,  1.1225],\n",
      "        [ 4.2981, -0.4974]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a.requires_grad_(True)  # .requires_grad_(True)ë¡œ ì¸í•´ ê¸°ì¡´ì˜ requires_grad=Falseê°€ Trueë¡œ ë°”ë€Œì—ˆë‹¤.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>requires_grad</code>ê°€ Trueë¡œ ë³€ê²½ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<SumBackward0 object at 0x000001B2C8E1BFC8>\n"
     ]
    }
   ],
   "source": [
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)  # bì˜ grad_fnì€ Tensorë¥¼ ìƒì„±í•œ Functionì„ ì°¸ì¡°í•œë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì—­ì „íŒŒ(backprop)ë„ í•´ë³´ì.\n",
    "ì•ì„œ ì •ì˜í•œ ë³€ìˆ˜ <code>out</code>ì€ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ê°’ë§Œ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— <code>out.backward()</code>ëŠ” <code>out.backward(torch.tensor(1.))</code>ê³¼ ë™ì¼í•˜ë‹¤.<br>\n",
    "(outì€ zì˜ í‰ê· ê°’ìœ¼ë¡œ ìœ„ì—ì„œ êµ¬í˜„í•´ ë‘ì—ˆë‹¤) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë³€í™”ë„ d(out)/dxë¥¼ ì¶œë ¥í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë²¡í„°-ì•¼ì½”ë¹„ì•ˆ ê³±ì˜ ì˜ˆì œë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì¼ë°˜ì ìœ¼ë¡œ , <code>torch.autograd</code>ëŠ” ë²¡í„°-ì•¼ì½”ë¹„ì•ˆ ê³±ì„ ê³„ì‚°í•˜ëŠ” ì—”ì§„ì´ë‹¤. \n",
    "\n",
    "    ì¦‰, ì–´ë–¤ ë²¡í„° v=(v1v2â‹¯vm)T ì— ëŒ€í•´ vTâ‹…J ì„ ì—°ì‚°í•©ë‹ˆë‹¤. ë§Œì•½ v ê°€ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ l=g(yâƒ— ) ì˜ ê¸°ìš¸ê¸°ì¸ ê²½ìš°, v=(âˆ‚lâˆ‚y1â‹¯âˆ‚lâˆ‚ym)T ì´ë©°, ì—°ì‡„ë²•ì¹™(chain rule)ì— ë”°ë¼ ë²¡í„°-ì•¼ì½”ë¹„ì•ˆ ê³±ì€ xâƒ—  ì— ëŒ€í•œ l ì˜ ê¸°ìš¸ê¸°ê°€ ë©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([0.4175, 1.4641, 0.6905], requires_grad=True)\n",
      "y:  tensor([ 427.5493, 1499.2018,  707.1223], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x: ', x)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print('y: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>torch.autograd</code>ëŠ” ì „ì²´ ì•¼ì½”ë¹„ì•ˆì„ ì§ì ‘ ê³„ì‚°í• ìˆ˜ëŠ” ì—†ì§€ë§Œ, ë²¡í„°-ì•¼ì½”ë¹„ì•ˆ ê³±ì€ ê°„ë‹¨íˆ <code>backward</code>ì— í•´ë‹¹ ë²¡í„°ë¥¼ ì¸ìë¡œ ì œê³µí•˜ì—¬ ì–»ì„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Jacobian-Matrixë€?\n",
    "\n",
    "* cs231nì—ì„œë„ ë§¤ìš° í˜¼ë™ë˜ì—ˆë˜ ê°œë…!!  - 20.03.31.Tue - \n",
    "    * <code>ì±„ì›Œë„£ê¸°</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. ì‹ ê²½ë§ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ê²½ë§ì€ <code>torch.nn</code>íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>nn</code>ì€ ëª¨ë¸ì„ ì •ì˜í•˜ê³  ë¯¸ë¶„í•˜ëŠ”ë° <code>autograd</code>ë¥¼ ì‚¬ìš©í•œë‹¤. <code>nn.Module</code>ì€ ê³„ì¸µ(layer)ê³¼ <code>output</code>ì„ ë°˜í™˜í•˜ëŠ” <code>forward(input)</code>ë©”ì„œë“œë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆ ì ˆì—ì„œëŠ”...<br>\n",
    "ê°„ë‹¨í•œ <code>ìˆœì „íŒŒ ë„¤íŠ¸ì›Œí¬(Feed-forward network)</code>ë¥¼ êµ¬í˜„í•œë‹¤. ì…ë ¥(input)ì„ ë°›ì•„ ì—¬ëŸ¬ ê³„ì¸µì— ì°¨ë¡€ë¡œ ì „ë‹¬í•œ í›„, ìµœì¢… ì¶œë ¥(output)ì„ ì œê³µí•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì‹ ê²½ë§ì˜ ì¼ë°˜ì ì¸ í•™ìŠµê³¼ì •\n",
    "    * <code>ì±„ì›Œë„£ê¸°</code>\n",
    "    * í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ (ë˜ëŠ” ê°€ì¤‘ì¹˜(weight))ë¥¼ ê°–ëŠ” ì‹ ê²½ë§ì„ ì •ì˜í•œë‹¤.\n",
    "    * ë°ì´í„°ì…‹(dataset) ì…ë ¥ì„ ë°˜ë³µí•œë‹¤.\n",
    "    * ì…ë ¥ì„ ì‹ ê²½ë§ì—ì„œ ì „íŒŒ(process)í•œë‹¤.\n",
    "    * ì†ì‹¤(loss; ì¶œë ¥ì´ ì •ë‹µìœ¼ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€)ì„ ê³„ì‚°í•œë‹¤. \n",
    "    * ë³€í™”ë„(gradient)ë¥¼ ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ë“¤ì— ì—­ìœ¼ë¡œ ì „íŒŒí•œë‹¤.\n",
    "    * ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒì˜ ê°„ë‹¨í•œ ê·œì¹™ì„ ì‚¬ìš©í•œë‹¤.\n",
    "    * <code>ê°€ì¤‘ì¹˜(weight) = ê°€ì¤‘ì¹˜(weight) - í•™ìŠµë¥ (learning rate) * ë³€í™”ë„(gradient)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channer, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        \n",
    "        # ì°¸ê³  https://github.com/inmoonlight/PyTorchTutorial/blob/master/01_CNN.ipynb\n",
    "        \n",
    "                                         # í¬í•œí•˜ë„¤ \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)  # Conv2d í†µê³¼ í›„ receptive field ê°’.\n",
    "                                         # ì´ˆê¸° ê°’ì´ ê·¸ë¦¼ê³¼ ê°™ì´ 28*28ì¼ ë•Œ,  6@26*26\n",
    "                                         # íŠœí† ë¦¬ì–¼ì´ë¼ ê·¸ëŸ°ì§€ poolë ˆì´ì–´ê°€ ìƒëµë˜ì–´ìˆë‹¤(ì—¬ê¸°ì„œ í˜¼ë™ì´ ì˜¨ë“¯)\n",
    "                                         # 6@13*13\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) # 16@11*11\n",
    "                                         # 16@6*6\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6 * 6 from Image dimension # ???\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "                             # batch ì°¨ì›ì„ ì œì™¸í•œ ì°¨ì›ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        num_features = 1  # ì¼ì¢…ì˜ flat feature ì´ˆê¸°í™” ê°™ì€\n",
    "        for s in size:\n",
    "            num_flat_features *= s\n",
    "        return num_flat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # 3ê°œì˜ ë©”ì„œë“œ(method)ë¥¼ ìƒì„±í•œë‹¤.\n",
    "    # __init__(), forward(), num_flat_features()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wb + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)   # 6*6 from image dimension(receptive field)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>forward</code>í•¨ìˆ˜ë§Œ ì •ì˜í•˜ê³  ë‚˜ë©´, (ë³€í™”ë„ë¥¼ ê³„ì‚°í•˜ëŠ”) <code>backward</code>í•¨ìˆ˜ëŠ” <code>autograd</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì •ì˜ëœë‹¤. <code>forward</code>í•¨ìˆ˜ì—ì„œëŠ” ì–´ë– í•œ Tensor ì—°ì‚°ì„ ì‚¬ìš©í•´ë„ ëœë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë“¤ì€ <code>net.parameters()</code>ì— ì˜í•´ ë°˜í™˜ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„ì˜ì˜ 32x32 ì…ë ¥ê°’ì„ ë„£ì–´ë³¸ë‹¤.<br>\n",
    "**Note** <br>\n",
    "ì´ ì‹ ê²½ë§(LeNet)ì˜ ì˜ˆìƒë˜ëŠ” ì…ë ¥ì˜ í¬ê¸°ëŠ” 32x32ì´ë‹¤.<br>\n",
    "ì´ ì‹ ê²½ë§ì— MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 32x32ë¡œ ë³€ê²½í•´ì•¼ í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ ê·¸ëŸ¬ë©´ ìœ„ì—ì„œ self.f1 = nn.Linear(16 * 6 * 6, 120)ì˜ 6 * 6ì€ ëŒ€ì²´ ë­ì•¼??<br>\n",
    "Input Image -> Conv -> ReLU -> Conv -> ReLU -> <code>fc1</code> ì´ ë•Œ, receptive fieldì˜ í¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0839, -0.0825,  0.0231, -0.0565, -0.0311, -0.0410, -0.0039,  0.0912,\n",
      "          0.0668,  0.0066]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë“  ë§¤ê°œë³€ìˆ˜ì˜ ë³€í™”ë„ ë²„í¼(gradient buffer)ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ê³ , ë¬´ì‘ìœ„ ê°’ìœ¼ë¡œ ì—­ì „íŒŒë¥¼ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note\n",
    "    * <code>torch.nn</code>ì€ ë¯¸ë‹ˆ-ë°°ì¹˜(mini-batch)ë§Œ ì§€ì›í•œë‹¤.\n",
    "    * ì¦‰ í•˜ë‚˜ì˜ ìƒ˜í”Œì´ ì•„ë‹Œ, ìƒ˜í”Œë“¤ì˜ ë¯¸ë‹ˆ-ë°°ì¹˜ë§Œì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.\n",
    "    * ì˜ˆë¥¼ ë“¤ì–´, <code>nnConv2DëŠ”</code> <code>nSamples x nChannels x Height x Width</code> ì˜ 4ì°¨ì› Tensorë¥¼ ì…ë ¥ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n",
    "    * ë§Œì•½ í•˜ë‚˜ì˜ ìƒ˜í”Œë§Œ ìˆë‹¤ë©´, <code>input.unsqueeze(0)</code> ì„ ì‚¬ìš©í•´ì„œ ê°€ì§œ ì°¨ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”– ë” ì‚´í´ë³¼ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.<br>\n",
    "   * ì†ì‹¤ì„ ê³„ì‚°í•˜ëŠ” ê²ƒ\n",
    "   * ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•˜ëŠ” ê²ƒ\n",
    "   ---\n",
    "   20.04.02.Thur.pm5:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì†ì‹¤ í•¨ìˆ˜(Loss Funciton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0104, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000001E54B850A88>\n",
      "<AddmmBackward object at 0x000001E54B9D9E08>\n",
      "<AccumulateGrad object at 0x000001E54B9E8A08>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSE loss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—­ì „íŒŒ(Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜¤ì°¨ë¥¼ ì—­ì „íŒŒí•˜ê¸° ìœ„í•´ì„œëŠ” <code>loss.backward()</code>ë§Œ í•´ì£¼ë©´ ëœë‹¤. <br>\n",
    "ê¸°ì¡´ ë³€í™”ë„ë¥¼ ì—†ì• ëŠ” ì‘ì—…ì´ í•„ìš”í•œë°, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë³€í™”ë„ê°€ ê¸°ì¡´ì˜ ê²ƒì— ëˆ„ì ë˜ê¸° ë•Œë¬¸ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad afer backward\n",
      "tensor([ 0.0075,  0.0045, -0.0167,  0.0021,  0.0055,  0.0044])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()  # zeros the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad afer backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°€ì¤‘ì¹˜ ê°±ì‹ \n",
    "ì‹¤ì œë¡œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ê°±ì‹  ê·œì¹™ì€ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD; Stochastic Gradient Descent)ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>ê°€ì¤‘ì¹˜(weight) = ê°€ì¤‘ì¹˜(weight) - í•™ìŠµë¥ (learning rate) * ë³€í™”ë„(gradient) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ê²½ë§ì„ êµ¬ì„±í•  ë•Œ ë‹¤ì–‘í•œ ê°±ì‹  ê·œì¹™ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆë‹¤.(e.g SGD, Nesterov-SGD, Adam, RMSProp ë“±...)<br>\n",
    "ì´ë¥¼ ìœ„í•´ <code>torch.optim</code> ë¼ëŠ” ì‘ì€ íŒ¨í‚¤ì§€ì— ì´ëŸ¬í•œ ë°©ë²•ë“¤ì„ ëª¨ë‘ êµ¬í˜„í•´ë‘ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Optimizerë¥¼ ìƒì„±í•œë‹¤.\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# í•™ìŠµ ê³¼ì •(training loop)ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "optimizer.zero_grad()  # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()  # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ—’ **Note**<br>\n",
    "<code>optimizer.zero_grad()</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ ë³€í™”ë„ ë²„í¼ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì— ìœ ì˜í•˜ì. ì´ëŠ” ì—­ì „íŒŒ(Backprop) ì„¹ì…˜ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ë³€í™”ë„ê°€ ëˆ„ì ë˜ê¸° ë•Œë¬¸ì´ë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ**VSCodeì—ì„œ í•œë²ˆì— êµ¬í˜„í•˜ê¸°**<br>\n",
    "ë‹¤ì‹œí•œë²ˆ êµ¬í˜„í•˜ë©´ì„œ í•™ìŠµ ë°©ë²•ì„ ìµíˆì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
